# **TRANSFORMERS FOR NATURAL LANGUAGE PROCESSING**

The repository contains a list of the projects and notebooks which we have worked on while reading the book **Transformers for Natural Language Processing**.

## **ðŸ“šNOTEBOOKS:**

[**1. TRANSFORMER ARCHITECTURE**](https://github.com/ThinamXx/Transformers_NLP/tree/main/01.%20Transformers%20for%20NLP/01.%20Transformer%20Architecture) 
- The **Positional Encoding** notebook contains information about Positional Encoding, Word Embeddings and Positional Vectors. The **Transformer Architecture** notebook contains information about Input Embedding, Multi-head Attention, Weight Matrices, Matrix Multiplication and Attention Representations.

[**2. FINE-TUNING BERT MODEL**](https://github.com/ThinamXx/Transformers_NLP/tree/main/01.%20Transformers%20for%20NLP/02.%20Fine-Tuning%20BERT%20Model)
- The **BERT** notebook contains information about BERT Architecture, Processing Dataset, BERT Tokenizer, Attention Masks, BERT Model Configuration, Optimizer Grouped Parameters, Initializing Hyperparameters, Tranining & Evaluation and Matthews Correlation Coefficient.

[**3. PRETRAINING ROBERTA MODEL**](https://github.com/ThinamXx/Transformers_NLP/tree/main/01.%20Transformers%20for%20NLP/03.%20Pretraining%20RoBERTa%20Model)
- The **RoBERTa** Model notebook contains information about Loading Dataset, Training a Tokenizer, Saving & Loading Trained Tokenizer, Model Configurations, Model Parameters, Data Collator, Transformer Trainer and Pretraining Language Modeling.

[**4. NLP TASKS WITH TRANSFORMERS**](https://github.com/ThinamXx/Transformers_NLP/tree/main/01.%20Transformers%20for%20NLP/04.%20NLP%20Tasks%20with%20Transformers) 
- The **Transformers** notebook contains information about Transformers, Corpus of Linguistic Acceptability, Stanford Sentiment Treebank, Microsoft Research Paraphrase Corpus and Winograd Schemas.

[**5. MACHINE TRANSLATION**](https://github.com/ThinamXx/Transformers_NLP/tree/main/01.%20Transformers%20for%20NLP/05.%20Machine%20Translation) 
- The **Machine Translation** notebook contains information about Machine Translation, Preprocessing Dataset, Vocabulary & OOV Words, Bilingual Evaluation Understudy Score, Trax & Transformer Model, Tokenization and Decoding.

[**6. TEXT GENERATION WITH GPT-2**](https://github.com/ThinamXx/Transformers_NLP/tree/main/01.%20Transformers%20for%20NLP/06.%20Text%20Generation%20GPT-2)
- The **GPT2 Model** notebook contains information about OpenAI GPT2 Model, Encoding and Training GPT2 Model, Transformers, Reformers, PET, Text Completion and Language Models.

[**7. T5 MODEL**](https://github.com/ThinamXx/Transformers_NLP/blob/main/01.%20Transformers%20for%20NLP/07.%20Summarization%20with%20T5/T5Model.ipynb)
- The T5 Model notebook contains information about Architecture of T5 Model, Encoder and Decoder Blocks, Summarizing Documents with T5 Model and Transformer Models.
