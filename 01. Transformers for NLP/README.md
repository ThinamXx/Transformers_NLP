# **TRANSFORMERS FOR NATURAL LANGUAGE PROCESSING**

The repository contains a list of the projects and notebooks which we have worked on while reading the book **Transformers for Natural Language Processing**.

### **ðŸ“šNOTEBOOKS:**

[**1. TRANSFORMER ARCHITECTURE**](https://github.com/ThinamXx/Transformers_NLP/tree/main/01.%20Transformers%20for%20NLP/01.%20Transformer%20Architecture) 
- The **Positional Encoding** notebook contains information about Positional Encoding, Word Embeddings and Positional Vectors. The **Transformer Architecture** notebook contains information about Input Embedding, Multi-head Attention, Weight Matrices, Matrix Multiplication and Attention Representations.

[**2. FINE-TUNING BERT MODEL**](https://github.com/ThinamXx/Transformers_NLP/tree/main/01.%20Transformers%20for%20NLP/02.%20Fine-Tuning%20BERT%20Model)
- The **BERT** notebook contains information about BERT Architecture, Processing Dataset, BERT Tokenizer, Attention Masks, BERT Model Configuration, Optimizer Grouped Parameters, Initializing Hyperparameters, Tranining & Evaluation and Matthews Correlation Coefficient.

[**3. PRETRAINING ROBERTA MODEL**](https://github.com/ThinamXx/Transformers_NLP/tree/main/01.%20Transformers%20for%20NLP/03.%20Pretraining%20RoBERTa%20Model)
- The **RoBERTa** Model notebook contains information about Loading Dataset, Training a Tokenizer, Saving & Loading Trained Tokenizer, Model Configurations, Model Parameters, Data Collator, Transformer Trainer and Pretraining Language Modeling.
