{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Initialization**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ],
      "metadata": {
        "id": "_FtT6tSyRVkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "7PK0xvgaReQQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Libraries and Dependencies**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ],
      "metadata": {
        "id": "1mquntmBRjSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPORTING MODULES: UNCOMMENT BELOW:\n",
        "# !pip install transformers\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AdamW, BertForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#@ IGNORING WARNINGS: \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Qy5GMB_PRpYA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformers**\n",
        "- Transformers, like humans, acquire language understanding through a limited number of tasks. They detect connections through transduction and then generalize them through inductive operations. "
      ],
      "metadata": {
        "id": "GcDFlNoDAjun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Corpus of Linguistic Acceptability**\n",
        "- The goal is to evaluate the linguistic competence of an NLP model to judge the linguistic acceptability of a sentence. The NLP model is expected to classify the sentence accordingly. "
      ],
      "metadata": {
        "id": "epmpATdYR9ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ LOADING THE DATASET:\n",
        "PATH = \"/content/drive/MyDrive/Data/in_domain_train.tsv\"                            # Path to dataset. \n",
        "df = pd.read_csv(PATH, delimiter=\"\\t\", header=None,\n",
        "                 names=[\"sentence_source\", \"label\", \"label_notes\", \"sentence\"])     # Reading the dataset.\n",
        "df.shape                                                                            # Inspecting dataset."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cmep7nB_R9I6",
        "outputId": "65608dd7-cd27-40b3-e059-72f382dc18d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8551, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrW7AgT_ATMK",
        "outputId": "3e833b8a-2e40-4f9e-dc8f-33804e801288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#@ LOADING PRETRAINED BERT MODEL:\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
        "                                                      num_labels=2)                 # Initializing pretrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stanford Sentiment TreeBank**\n",
        "- **SST-2** contains movie reviews."
      ],
      "metadata": {
        "id": "meadoOnzUjr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ SST-2 BINARY CLASSIFICATION:\n",
        "nlp = pipeline(\"sentiment-analysis\")                                    # Initialization.\n",
        "print(nlp(\"If you sometimes like to go to the movies to have fun, \\\n",
        "           Wasabi is a good place to start.\"))                          # Inspecting sentiment."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQy5lVi7UUBr",
        "outputId": "2e49d25f-50af-4ed3-f770-b7be80855e03"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9998257756233215}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Microsoft Research Paraphrase Corpus**\n",
        "- The MRPC, a GLUE task, contains pairs of sentences extracted from new sources on the web. Each pair has been annotated by a human to indicate whether the sentences are equivalent based on two closely related properties: paraphrase equivalent and semantic equivalent."
      ],
      "metadata": {
        "id": "EhiWA3_GV04f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ SEQUENCE OR PARAPHRASE CLASSIFICATION:\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")                     # Initializing pretrained tokenizer. \n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")  # Initializing pretrained model. \n",
        "classes = [\"not paraphrase\", \"is paraphrase\"]                                                   # Initialization.\n",
        "sequence_A = \"The DVD-CCA then appealed to the state Supreme Court.\"                            # Initialization. \n",
        "sequence_B = \"The DVD CCA appealed that decision to the U.S. Supreme Court.\"                    # Initialization. \n",
        "paraphrase = tokenizer.encode_plus(sequence_A, sequence_B, return_tensors=\"tf\")\n",
        "paraphrase_classification_logits = model(paraphrase)[0]                                         # Implementation of model. \n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\n",
        "for i in range(len(classes)):\n",
        "    print(f\"{classes[i]}: {round(paraphrase_results[i]*100)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUrJk5bcVhHK",
        "outputId": "a5a4b011-d17d-4a7c-cce2-da1bf24d6c98"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing TFBertForSequenceClassification: ['dropout_183']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at bert-base-cased-finetuned-mrpc.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not paraphrase: 8%\n",
            "is paraphrase: 92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Winograd Schemas**"
      ],
      "metadata": {
        "id": "ATPYEwilZ68n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ WINOGRAD SCHEMAS:\n",
        "translator = pipeline(\"translation_en_to_fr\")                               # Initialization.\n",
        "translator(\"The car could not go in the garage because it was too big.\", \n",
        "           max_length=40)                                                   # Initializing translation."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzFmkk5gZeGp",
        "outputId": "9a5f4a9c-2787-41e8-8f51-8e4bbd289056"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to t5-base (https://huggingface.co/t5-base)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'translation_text': \"La voiture ne pouvait pas aller dans le garage parce qu'elle était trop grosse.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}