## **Pretraining RoBERTa Model**

The [**RoBERTa Model**](https://github.com/ThinamXx/Transformers_NLP/blob/main/01.%20Transformers%20for%20NLP/03.%20Pretraining%20RoBERTa%20Model/RoBERTa_Model.ipynb) notebook contains information about Loading Dataset, Training a Tokenizer, Saving & Loading Trained Tokenizer, Model Configurations, Model Parameters, Data Collator, Transformer Trainer and Pretraining Language Modeling.

**Note:**
- [**RoBERTa Model**](https://github.com/ThinamXx/Transformers_NLP/blob/main/01.%20Transformers%20for%20NLP/03.%20Pretraining%20RoBERTa%20Model/RoBERTa_Model.ipynb)

**Data Collator**
- A data collator will take samples from the dataset and collate them into batches.
