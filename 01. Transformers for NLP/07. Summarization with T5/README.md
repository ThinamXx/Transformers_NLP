## **Documents Summarization with T5 Model**

The [**T5 Model**](https://github.com/ThinamXx/Transformers_NLP/blob/main/01.%20Transformers%20for%20NLP/07.%20Summarization%20with%20T5/T5Model.ipynb) notebook contains information about Architecture of T5 Model, Encoder and Decoder Blocks, Summarizing Documents with T5 Model and Transformer Models. 

**Note:**
  - ðŸ“‘[**T5 Model**](https://github.com/ThinamXx/Transformers_NLP/blob/main/01.%20Transformers%20for%20NLP/07.%20Summarization%20with%20T5/T5Model.ipynb)

**T5 Architecture**
- The encoder and decoder layers of Transformers became blocks and sub-layers became sub-components containing self-attention layer and a feed forward network. Self-attention is order independent. It uses relative position embeddings.
