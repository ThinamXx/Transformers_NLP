{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransformerArchitecture.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Initialization**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ],
      "metadata": {
        "id": "JpcUsEH__RBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "-Dn9r4l3_jKB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Libraries and Dependencies**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ],
      "metadata": {
        "id": "pLcbyAp2_nT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING LIBRARIES AND DEPENDENCIES: \n",
        "import math"
      ],
      "metadata": {
        "id": "JccBJWLL_vSE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input Embedding**\n",
        "- The input embedding sub-layer converts the input tokens to vectors of dimension: 512 using learned embeddings in the original **Transformer** model. Cosine similarity uses Euclidean (L2) norm to create vectors in a unit sphere. "
      ],
      "metadata": {
        "id": "j6LH11N-2gYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Encoding**\n",
        "- The idea is to add a positional encoding value to the input embedding instead of having additional vectors to describe the position of the token in a sequence. "
      ],
      "metadata": {
        "id": "bDxwB7qU6fTR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QqOrucB7xe9_"
      },
      "outputs": [],
      "source": [
        "#@ IMPLEMENTATION OF POSITIONAL ENCODING:\n",
        "d_model = 512\n",
        "def positional_encoding(pos, pe):\n",
        "    for i in range(0, 512, 2):\n",
        "        pe[0][i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
        "        pc[0][i] = (y[0][i]*math.sqrt(d_model)) + pe[0][i]\n",
        "        pe[0][i+1] = math.cos(pos / (10000 ** ((2 * i) / d_model)))\n",
        "        pc[0][i+1] = (y[0][i+1]*math.sqrt(d_model)) + pe[0][i+1]\n",
        "    return pe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IyQKsBiC8jhO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}