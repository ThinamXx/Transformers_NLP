{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransformerArchitecture_T.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Initialization**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ],
      "metadata": {
        "id": "JpcUsEH__RBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "-Dn9r4l3_jKB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Libraries and Dependencies**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ],
      "metadata": {
        "id": "pLcbyAp2_nT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING LIBRARIES AND DEPENDENCIES: UNCOMMENT BELOW:\n",
        "# !pip -qq install transformers\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "JccBJWLL_vSE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input Embedding**\n",
        "- The input embedding sub-layer converts the input tokens to vectors of dimension: 512 using learned embeddings in the original **Transformer** model. Cosine similarity uses Euclidean (L2) norm to create vectors in a unit sphere. "
      ],
      "metadata": {
        "id": "j6LH11N-2gYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Encoding**\n",
        "- The idea is to add a positional encoding value to the input embedding instead of having additional vectors to describe the position of the token in a sequence. "
      ],
      "metadata": {
        "id": "bDxwB7qU6fTR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QqOrucB7xe9_"
      },
      "outputs": [],
      "source": [
        "#@ IMPLEMENTATION OF POSITIONAL ENCODING: EXAMPLE FUNCTION:\n",
        "d_model = 512\n",
        "def positional_encoding(pos, pe):\n",
        "    for i in range(0, 512, 2):\n",
        "        pe[0][i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
        "        pc[0][i] = (y[0][i]*math.sqrt(d_model)) + pe[0][i]\n",
        "        pe[0][i+1] = math.cos(pos / (10000 ** ((2 * i) / d_model)))\n",
        "        pc[0][i+1] = (y[0][i+1]*math.sqrt(d_model)) + pe[0][i+1]\n",
        "    return pe"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Architecture of Multi-Head Attention**"
      ],
      "metadata": {
        "id": "ryXNc_qetSKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ STEP 1: REPRESENT THE INPUT:\n",
        "print(\"Step 1: Input:3 inputs, d_model=4\")\n",
        "x = np.array([[1.0, 0.0, 1.0, 0.0],             # Input 1. \n",
        "              [0.0, 2.0, 0.0, 2.0],             # Input 2. \n",
        "              [1.0, 1.0, 1.0, 1.0]])            # Input 3. \n",
        "print(x)                                        # Inspection. "
      ],
      "metadata": {
        "id": "IyQKsBiC8jhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e07fcc0f-e5a5-4a77-e5ed-9329a2f29c5c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Input:3 inputs, d_model=4\n",
            "[[1. 0. 1. 0.]\n",
            " [0. 2. 0. 2.]\n",
            " [1. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ STEP 2: INITIALIZING WEIGHT MATRICES:\n",
        "print(\"Step 2: weights 3 dimensions x d_model=4\")\n",
        "print(\"w_query\")\n",
        "w_query = np.array([[1, 0, 1],\n",
        "                    [1, 0, 0],\n",
        "                    [0, 0, 1],\n",
        "                    [0, 1, 1]])                     # Initializing query weight matrix. \n",
        "print(w_query)                                      # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXfPr3K3uMx-",
        "outputId": "a57452ef-7e42-460b-9b5e-7e23c680fb6c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2: weights 3 dimensions x d_model=4\n",
            "w_query\n",
            "[[1 0 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ STEP 2: INITIALIZING WEIGHT MATRICES:\n",
        "print(\"w_key\")\n",
        "w_key = np.array([[0, 0, 1],\n",
        "                  [1, 1, 0],\n",
        "                  [0, 1, 0],\n",
        "                  [1, 1, 0]])                   # Initializing key weight matrix. \n",
        "print(w_key)                                    # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUMu8uAsvUPi",
        "outputId": "3c11646e-43f9-43e8-8dab-082d9d5d8ea4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_key\n",
            "[[0 0 1]\n",
            " [1 1 0]\n",
            " [0 1 0]\n",
            " [1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ STEP 2: INITIALIZING WEIGHT MATRICES:\n",
        "print(\"w_value\")\n",
        "w_value = np.array([[0, 2, 0],\n",
        "                    [0, 3, 0],\n",
        "                    [1, 0, 3],\n",
        "                    [1, 1, 0]])                 # Initializing value weight matrix. \n",
        "print(w_value)                                  # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqNVd7J1v4Mj",
        "outputId": "e8ff2d81-27e6-47c3-fb05-cce7dac264a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_value\n",
            "[[0 2 0]\n",
            " [0 3 0]\n",
            " [1 0 3]\n",
            " [1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ STEP 3: INITIALIZING MATRIX MULTIPLICATION: \n",
        "print(\"Step 3: Matrix multiplication to obtain Q,K,V\")\n",
        "print(\"Query: x * w_query\")\n",
        "Q = np.matmul(x, w_query)                               # Initializing matrix multiplication. \n",
        "print(Q)                                                # Inspection. "
      ],
      "metadata": {
        "id": "f788i6rewRoo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27516093-13af-433c-ca10-16712e423294"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3: Matrix multiplication to obtain Q,K,V\n",
            "Query: x * w_query\n",
            "[[1. 0. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 1. 3.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ STEP 3: INITIALIZING MATRIX MULTIPLICATION: \n",
        "print(\"Query: x * w_key\")\n",
        "K = np.matmul(x, w_key)                                 # Initializing matrix multiplication. \n",
        "print(K)                                                # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0BQ-ju5Uxp-",
        "outputId": "f231e42a-29f7-41ba-f311-ba62aa5a13b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: x * w_key\n",
            "[[0. 1. 1.]\n",
            " [4. 4. 0.]\n",
            " [2. 3. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ STEP 3: INITIALIZING MATRIX MULTIPLICATION: \n",
        "print(\"Query: x * w_value\")\n",
        "V = np.matmul(x, w_value)                               # Initializing matrix multiplication. \n",
        "print(V)                                                # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juRisq-9UyqT",
        "outputId": "233322c6-2847-4611-c4c0-527375bc960b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: x * w_value\n",
            "[[1. 2. 3.]\n",
            " [2. 8. 0.]\n",
            " [2. 6. 3.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ STEP 4: INITIALIZING ATTENTION SCORES: \n",
        "print(\"Step 4: Scaled Attention Scores\")\n",
        "k_d = 1                                                 # Square root of 3 and rounded down to 1. \n",
        "attention_scores = (Q @ K.transpose()) / k_d            # Getting attention scores. \n",
        "print(attention_scores)                                 # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pFQDZDXWOBT",
        "outputId": "1cd16e92-02f6-41bd-e0c4-cf1030846446"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4: Scaled Attention Scores\n",
            "[[ 2.  4.  4.]\n",
            " [ 4. 16. 12.]\n",
            " [ 4. 12. 10.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ STEP 5: SCALED SOFTMAX ATTENTION SCORES: \n",
        "print(\"Step 5: Scaled softmax attention scores\")\n",
        "attention_scores[0] = softmax(attention_scores[0])      # Implementation of softmax. \n",
        "attention_scores[1] = softmax(attention_scores[1])      # Implementation of softmax. \n",
        "attention_scores[2] = softmax(attention_scores[2])      # Implementation of sofmax. \n",
        "print(attention_scores[0])\n",
        "print(attention_scores[0])\n",
        "print(attention_scores[0])                              # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG0XiNAvX0Vt",
        "outputId": "d3b3b450-b09d-4779-9176-0060290da9e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5: Scaled softmax attention scores\n",
            "[0.06337894 0.46831053 0.46831053]\n",
            "[0.06337894 0.46831053 0.46831053]\n",
            "[0.06337894 0.46831053 0.46831053]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ STEP 6: FINAL ATTENTION REPRESENTATIONS: \n",
        "print(\"Step 6: attention value\")\n",
        "print(V[0])\n",
        "print(V[1])\n",
        "print(V[2])\n",
        "print(\"Attention 1\")\n",
        "attention1 = attention_scores[0].reshape(-1, 1)\n",
        "attention1 = attention_scores[0][0]*V[0]               # Finalizing attention score. \n",
        "print(attention1)                                      # Inspection. \n",
        "print(\"Attention 2\")\n",
        "attention2 = attention_scores[0][1]*V[1]               # Finalizing attention score. \n",
        "print(attention2)                                      # Inspection. \n",
        "print(\"Attention 3\")\n",
        "attention3 = attention_scores[0][2]*V[2]               # Finalizing attention score. \n",
        "print(attention3)                                      # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oofLAvikK3g",
        "outputId": "6ac14b65-9442-416b-816b-43899bb21beb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6: attention value\n",
            "[1. 2. 3.]\n",
            "[2. 8. 0.]\n",
            "[2. 6. 3.]\n",
            "Attention 1\n",
            "[0.06337894 0.12675788 0.19013681]\n",
            "Attention 2\n",
            "[0.93662106 3.74648425 0.        ]\n",
            "Attention 3\n",
            "[0.93662106 2.80986319 1.40493159]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ STEP 7: SUMMING THE RESULTS: \n",
        "print(\"Step 7: Summing the results\")\n",
        "attention_input1 = attention1 + attention2 + attention3     # Summing the results. \n",
        "print(attention_input1)                                     # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gW8dRZjm6oU",
        "outputId": "08510339-7af5-450c-aae7-844e17306d80"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 7: Summing the results\n",
            "[1.93662106 6.68310531 1.59506841]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ STEP 8: INITIALIZING ATTENTION REPRESENTATIONS: EXAMPLE: \n",
        "attention_head1 = np.random.random((3, 64))                 # Initialization. \n",
        "print(attention_head1)                                      # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxMbZ7H0nyyB",
        "outputId": "ed40b8af-4bec-4e1a-c26f-58b553eac00e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.21508027e-01 5.76604901e-01 3.95303427e-01 7.21838988e-01\n",
            "  3.29573122e-01 2.05844016e-01 7.16550605e-01 5.62129504e-01\n",
            "  5.11750850e-02 4.90412180e-01 2.09863177e-01 9.07462798e-01\n",
            "  1.53313112e-01 9.72728410e-01 2.00046114e-01 2.48139566e-01\n",
            "  3.81248851e-01 8.22423442e-01 6.98415964e-01 6.48853234e-02\n",
            "  9.49851033e-01 4.67558156e-01 3.23587396e-01 7.52059213e-01\n",
            "  5.42926451e-01 7.16783847e-01 4.82497333e-01 7.86096799e-01\n",
            "  9.06572156e-01 6.86148716e-01 5.75597690e-01 1.53192450e-01\n",
            "  4.92509506e-01 1.82668613e-01 1.96318237e-01 8.13300755e-01\n",
            "  1.52368538e-01 4.87183201e-01 3.92922877e-01 6.38783502e-01\n",
            "  7.20154053e-01 3.93365940e-01 2.71397320e-01 8.45178738e-01\n",
            "  2.77496811e-01 5.99147790e-01 8.84879036e-01 7.66972633e-02\n",
            "  7.11229421e-01 1.08425179e-01 3.35591945e-01 7.12204598e-01\n",
            "  8.67933687e-02 9.60378424e-01 5.67299883e-01 9.98373145e-01\n",
            "  3.52370191e-02 6.57861893e-01 5.00225797e-01 5.59332322e-02\n",
            "  7.10973447e-02 4.09312887e-02 4.89027695e-01 8.79644598e-01]\n",
            " [6.87968739e-01 6.22215402e-01 4.85174539e-01 2.12113945e-01\n",
            "  2.82488007e-01 5.88779504e-01 7.72063334e-01 2.57347462e-01\n",
            "  2.70066398e-02 7.34701103e-01 5.07631788e-01 4.75390936e-01\n",
            "  3.36643092e-01 7.02332324e-01 9.42520610e-01 9.02888202e-01\n",
            "  7.36545549e-01 4.52365348e-01 9.89398950e-01 1.92269608e-01\n",
            "  6.58614956e-01 2.18990253e-01 3.00787171e-01 3.89369839e-01\n",
            "  6.24114470e-01 7.46644590e-01 7.78925636e-01 2.79588133e-01\n",
            "  1.78840551e-01 1.92872860e-01 4.44043624e-01 4.83326623e-01\n",
            "  3.41259488e-01 9.10641488e-01 1.27081773e-01 3.50995890e-01\n",
            "  4.08424677e-01 8.25492722e-01 4.11941910e-02 9.89248264e-01\n",
            "  7.50706252e-01 7.55888646e-01 5.38012246e-01 3.18128633e-02\n",
            "  5.69776552e-01 1.17931083e-01 2.84043312e-01 2.09291438e-02\n",
            "  9.95864471e-01 7.97541890e-01 5.73337036e-01 8.71882563e-01\n",
            "  2.62672130e-01 2.59028152e-01 9.82232323e-01 5.03634426e-01\n",
            "  6.75247807e-01 4.72328158e-01 6.82962575e-01 8.09707969e-01\n",
            "  7.11477347e-01 5.58839208e-01 9.11781849e-01 8.57595199e-01]\n",
            " [7.67300926e-01 1.96412621e-01 3.92244700e-01 3.98803656e-01\n",
            "  1.54830659e-02 5.36369254e-01 9.57508674e-01 8.84258212e-01\n",
            "  3.48553314e-01 8.80812013e-01 9.13914954e-01 9.46865610e-01\n",
            "  3.29821089e-01 8.37436439e-02 2.03315310e-01 2.21359113e-01\n",
            "  9.95436860e-01 1.79575386e-01 9.42655986e-01 8.81181605e-01\n",
            "  9.18445219e-01 3.16900734e-01 4.90690431e-01 8.49299022e-01\n",
            "  3.47941902e-02 6.58901060e-01 1.80137936e-01 9.61206482e-01\n",
            "  1.76602232e-01 7.64305029e-01 3.07227738e-01 6.74783929e-01\n",
            "  6.50118697e-01 2.93634365e-02 2.31534733e-01 5.28685411e-01\n",
            "  1.65484070e-01 1.42875642e-01 2.72063325e-01 9.95009189e-01\n",
            "  2.36823646e-01 1.30643344e-01 5.52861230e-01 9.02417505e-01\n",
            "  2.69065154e-01 6.90444439e-01 4.30928162e-01 5.04278589e-01\n",
            "  9.65243794e-01 4.85645263e-02 3.85643610e-01 5.22153596e-01\n",
            "  2.49819919e-01 9.68635788e-01 7.95746635e-01 2.32122808e-01\n",
            "  3.88796285e-01 4.53740783e-01 7.09250217e-02 9.60995542e-01\n",
            "  4.23944072e-01 8.10556342e-01 4.01014272e-04 5.50046703e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ STEP 9: INITIALIZING TRAINED HEADS OF ATTENTION SUB-LAYER:\n",
        "print(\"Step 9: Initializing 8 heads of attention sub-layer\")\n",
        "z0h1=np.random.random((3, 64))\n",
        "z1h2=np.random.random((3, 64))\n",
        "z2h3=np.random.random((3, 64))\n",
        "z3h4=np.random.random((3, 64))\n",
        "z4h5=np.random.random((3, 64))\n",
        "z5h6=np.random.random((3, 64))\n",
        "z6h7=np.random.random((3, 64))\n",
        "z7h8=np.random.random((3, 64))\n",
        "print(\"Shape of one head\", z0h1.shape)\n",
        "print(\"Dimension of 8 heads\", 64 * 8)"
      ],
      "metadata": {
        "id": "s46mfkGEqchJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa99babb-dd95-41a8-fc30-e434787caff3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 9: Initializing 8 heads of attention sub-layer\n",
            "Shape of one head (3, 64)\n",
            "Dimension of 8 heads 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ STEP 10: CONCATENATION OF OUTPUT OF HEADS:\n",
        "print(\"Step 10: Concatenation of heads 1 to 8\")\n",
        "output_attention = np.hstack((z0h1,z1h2,z2h3,z3h4,z4h5,z5h6,z6h7,z7h8))     # Initializing concatenation. \n",
        "print(output_attention)                                                     # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2E2CrbVsUiH",
        "outputId": "f8308e02-f262-4969-d831-f4c4bf7eced6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10: Concatenation of heads 1 to 8\n",
            "[[0.45774176 0.26904581 0.00950891 ... 0.64688242 0.97711683 0.83310905]\n",
            " [0.42674038 0.47108581 0.09232858 ... 0.81539069 0.71924008 0.55766871]\n",
            " [0.03559094 0.39032275 0.79041977 ... 0.48059656 0.73965045 0.22084276]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF HUGGING FACE TRANSFORMERS:\n",
        "translator = pipeline(\"translation_en_to_fr\")            # Initializing Translator. \n",
        "print(translator(\"I am Thinam Tamang.\"))                 # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gkzQWaAt-ur",
        "outputId": "a6e9924a-1b4e-45ca-feb3-4e9d68104de5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to t5-base (https://huggingface.co/t5-base)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'translation_text': 'Je suis Thinam Tamang.'}]\n"
          ]
        }
      ]
    }
  ]
}