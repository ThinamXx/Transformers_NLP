## **Machine Translation with Transformers**

The [**Machine Translation**](https://github.com/ThinamXx/Transformers_NLP/blob/main/01.%20Transformers%20for%20NLP/05.%20Machine%20Translation/MachineTranslation.ipynb) notebook contains information about Machine Translation, Preprocessing Dataset, Vocabulary & OOV Words, Bilingual Evaluation Understudy Score, Trax & Transformer Model, Tokenization and Decoding.

**Note:**
  - üìù[**Machine Translation**](https://github.com/ThinamXx/Transformers_NLP/blob/main/01.%20Transformers%20for%20NLP/05.%20Machine%20Translation/MachineTranslation.ipynb)
  - The [**read.py**](https://github.com/ThinamXx/Transformers_NLP/blob/main/01.%20Transformers%20for%20NLP/05.%20Machine%20Translation/read.py) contains information about processing the dataset.
  - The [**read_clean.py**](https://github.com/ThinamXx/Transformers_NLP/blob/main/01.%20Transformers%20for%20NLP/05.%20Machine%20Translation/read_clean.py) contains information about preprocessing the dataset. 
  - The [**BLEU.py**](https://github.com/ThinamXx/Transformers_NLP/blob/main/01.%20Transformers%20for%20NLP/05.%20Machine%20Translation/BLEU.py) contains information about Bilingual Evaluation Understudy Score.


**Machine Translation**
- Machine translation is the process of reproducing human translation by machine transductions and outputs. The transduction process of the original Transformer architecture uses the encoder, the decoder stack, and all of the model's parameters to represent a reference sequence.
