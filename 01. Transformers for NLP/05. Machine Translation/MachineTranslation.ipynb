{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineTranslation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Initialization**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ],
      "metadata": {
        "id": "ud_YC-1DhzIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "NI0uUYc-h6rl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine Translation**\n",
        "- Machine translation is the process of reproducing human translation by machine transductions and outputs. The transduction process of the original Transformer architecture uses the encoder, the decoder stack, and all of the model's parameters to represent a reference sequence. "
      ],
      "metadata": {
        "id": "d42CZ7CYdra9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Processing Dataset**"
      ],
      "metadata": {
        "id": "4ytWnPzZmOLj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKocEbr-cUrj",
        "outputId": "7fa29da2-cafc-42cb-a3db-b7bfdbe05806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing read.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile read.py\n",
        "#@ PROCESSING THE DATASET:\n",
        "import pickle\n",
        "from pickle import dump\n",
        "\n",
        "import re \n",
        "import string\n",
        "import unicodedata\n",
        "\n",
        "#@ LOADING INTO MEMORY:\n",
        "def load_doc(filename):                                 # Defining function.\n",
        "    file = open(filename, mode=\"rt\", encoding=\"utf-8\")  # Opening file.\n",
        "    text = file.read()                                  # Reading file.\n",
        "    file.close()                                        # Closing file.\n",
        "    return text                                         # Getting all texts.\n",
        "\n",
        "#@ SPLITTING INTO SENTENCES:\n",
        "def to_sentence(doc):                                   # Defining function.\n",
        "    return doc.strip().split(\"\\n\")                      # Splitting into sentences. \n",
        "\n",
        "#@ SHORTEST AND LONGEST SENTENCES:\n",
        "def sentence_lengths(sentences):                        # Defining function.\n",
        "    lengths = [len(s.split()) for s in sentences]       # Lengths of sentence.\n",
        "    return min(lengths), max(lengths)                   # Getting minimum and maximum.\n",
        "\n",
        "#@ CLEANING SENTENCES:\n",
        "def clean_lines(lines):                                            # Defining function.\n",
        "    cleaned = list()                                               # Initialization. \n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))   # Preparing regex.\n",
        "    table = str.maketrans('', '', string.punctuation)              # Removing punctuation. \n",
        "    for line in lines:\n",
        "        line = unicodedata.normalize(\n",
        "            \"NFD\", line).encode(\"ascii\", \"ignore\")                 # Normalizing unicode.\n",
        "        line = line.decode(\"UTF-8\")\n",
        "        line = line.split()                                        # Tokenization.\n",
        "        line = [word.lower() for word in line]                     # Lower case.\n",
        "        line = [word.translate(table) for word in line]            # Remove punctuation.\n",
        "        line = [re_print.sub('', w) for w in line]                 # Remove non-printable.\n",
        "        line = [word for word in line if word.isalpha()]           # Remove numbers.\n",
        "        cleaned.append(' '.join(line))                             # Storing as string.\n",
        "    return cleaned \n",
        "\n",
        "#@ LOADING ENGLISH DATA:\n",
        "path_file_en = \"/content/drive/MyDrive/Data/europarl-v7.fr-en.en\"  # English data.\n",
        "doc = load_doc(path_file_en)                                       # Loading.\n",
        "sentences = to_sentence(doc)                                       # Splitting into sentences.\n",
        "minlen, maxlen = sentence_lengths(sentences)                       # Shortest and longest.\n",
        "print(\"English data: sentences=%d, min=%d, max=%d\" %(\n",
        "    len(sentences), minlen, maxlen))\n",
        "cleanf = clean_lines(sentences)                                    # Cleaning sentences.\n",
        "filename = \"English.pkl\"                                           # Initialization.\n",
        "outfile = open(filename, \"wb\")\n",
        "pickle.dump(cleanf, outfile)                                       # Storing.\n",
        "outfile.close()                                                    # Closing.\n",
        "print(filename, \" saved\")                                          # Inspection. \n",
        "\n",
        "#@ LOADING FRENCH DATA:\n",
        "path_file_fr = \"/content/drive/MyDrive/Data/europarl-v7.fr-en.fr\"  # French data.\n",
        "doc = load_doc(path_file_fr)                                       # Loading.\n",
        "sentences = to_sentence(doc)                                       # Splitting into sentences.\n",
        "minlen, maxlen = sentence_lengths(sentences)                       # Shortest and longest.\n",
        "print(\"French data: sentences=%d, min=%d, max=%d\" %(\n",
        "    len(sentences), minlen, maxlen))\n",
        "cleanf = clean_lines(sentences)                                    # Cleaning sentences.\n",
        "filename = \"French.pkl\"                                            # Initialization.\n",
        "outfile = open(filename, \"wb\")\n",
        "pickle.dump(cleanf, outfile)                                       # Storing.\n",
        "outfile.close()                                                    # Closing.\n",
        "print(filename, \" saved\")                                          # Inspection. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ PROCESSING THE DATA: \n",
        "!python read.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmjhzyFvuM96",
        "outputId": "71bf7992-fbff-4a75-a614-6b781cf71a08"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English data: sentences=2007723, min=0, max=668\n",
            "English.pkl  saved\n",
            "French data: sentences=2007723, min=0, max=693\n",
            "French.pkl  saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing Dataset**"
      ],
      "metadata": {
        "id": "Wi_de0Iyvsep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile read_clean.py\n",
        "#@ PREPROCESSING THE DATASET:\n",
        "\n",
        "import pickle\n",
        "from pickle import load\n",
        "from pickle import dump\n",
        "from collections import Counter\n",
        "\n",
        "#@ LOADING CLEAN DATASET:\n",
        "def load_clean_sentences(filename):         # Defining function.\n",
        "    return load(open(filename, \"rb\"))       # Loading clean dataset.\n",
        "\n",
        "#@ SAVING CLEAN DATASET:\n",
        "def save_clean_sentences(sentences, filename):      # Defining function. \n",
        "    dump(sentences, open(filename, \"wb\"))           # Saving dataset. \n",
        "    print(\"Saved: %s\" % filename)                   # Inspection.\n",
        "\n",
        "#@ CREATING SEQUENCE TABLE: VOCABULARY:\n",
        "def to_vocab(lines):                                # Defining function. \n",
        "    vocab = Counter()                               # Initializing counter.\n",
        "    for line in lines:\n",
        "        tokens = line.split()                       # Tokenization.\n",
        "        vocab.update(tokens)                        # Updating.\n",
        "    return vocab                                    # Getting vocabulary.\n",
        "\n",
        "#@ PREPROCESSING VOCABULARY:\n",
        "def trim_vocab(vocab, min_occurance):                               # Defining function.\n",
        "    tokens = [k for k,c in vocab.items() if c >= min_occurance]     # Trimming vocabulary tokens.\n",
        "    return set(tokens)                                              # Getting tokens.\n",
        "\n",
        "#@ PROCESSING OOV WORDS:\n",
        "def update_dataset(lines, vocab):                                   # Defining function.\n",
        "    new_lines = list()                                              # Initialization. \n",
        "    for line in lines:\n",
        "        new_tokens = list()                                         # Initialization.\n",
        "        for token in line.split():\n",
        "            if token in vocab:\n",
        "                new_tokens.append(token)\n",
        "            else:\n",
        "                new_tokens.append(\"unk\")                            # Adding unknown tokens.\n",
        "        new_line = ' '.join(new_tokens)\n",
        "        new_lines.append(new_line)\n",
        "    return new_lines                                                # Getting updated lines.\n",
        "\n",
        "#@ LOADING ENGLISH DATASET:\n",
        "filename = \"English.pkl\"                                            # Initialization.\n",
        "lines = load_clean_sentences(filename)                              # Loading.\n",
        "vocab = to_vocab(lines)                                             # Initializing vocabulary. \n",
        "print(\"English vocabulary: %d\" % len(vocab))                        # Inspection.\n",
        "vocab = trim_vocab(vocab, 5)                                        # Reducing vocabulary.\n",
        "print(\"New English vocabulary: %d\" % len(vocab))                    # Inspection. \n",
        "lines = update_dataset(lines, vocab)                                # Processing OOV.\n",
        "filename = \"english_vocab.pkl\"                                      # Initialization. \n",
        "save_clean_sentences(lines, filename)\n",
        "for i in range(10):\n",
        "    print(\"line\", i, \":\", lines[i])                                 # Inspection.\n",
        "\n",
        "#@ LOADING FRENCH DATASET:\n",
        "filename = \"French.pkl\"                                             # Initialization.\n",
        "lines = load_clean_sentences(filename)                              # Loading.\n",
        "vocab = to_vocab(lines)                                             # Initializing vocabulary. \n",
        "print(\"French vocabulary: %d\" % len(vocab))                         # Inspection.\n",
        "vocab = trim_vocab(vocab, 5)                                        # Reducing vocabulary.\n",
        "print(\"New French vocabulary: %d\" % len(vocab))                     # Inspection. \n",
        "lines = update_dataset(lines, vocab)                                # Processing OOV.\n",
        "filename = \"french_vocab.pkl\"                                       # Initialization. \n",
        "save_clean_sentences(lines, filename)\n",
        "for i in range(10):\n",
        "    print(\"line\", i, \":\", lines[i])                                 # Inspection."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQI4UjDIubiZ",
        "outputId": "4a460753-02f7-4bd2-d770-f70c765f5020"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing read_clean.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ PREPROCESSING THE DATASET: \n",
        "!python read_clean.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_cTc7Tn9hDJ",
        "outputId": "a23418f1-8342-41e2-a986-d0140dc466d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocabulary: 105357\n",
            "New English vocabulary: 41746\n",
            "Saved: english_vocab.pkl\n",
            "line 0 : resumption of the session\n",
            "line 1 : i declare resumed the session of the european parliament adjourned on friday december and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period\n",
            "line 2 : although as you will have seen the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful\n",
            "line 3 : you have requested a debate on this subject in the course of the next few days during this partsession\n",
            "line 4 : in the meantime i should like to observe a minute s silence as a number of members have requested on behalf of all the victims concerned particularly those of the terrible storms in the various countries of the european union\n",
            "line 5 : please rise then for this minute s silence\n",
            "line 6 : the house rose and observed a minute s silence\n",
            "line 7 : madam president on a point of order\n",
            "line 8 : you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka\n",
            "line 9 : one of the people assassinated very recently in sri lanka was mr unk unk who had visited the european parliament just a few months ago\n",
            "French vocabulary: 141642\n",
            "New French vocabulary: 58800\n",
            "Saved: french_vocab.pkl\n",
            "line 0 : reprise de la session\n",
            "line 1 : je declare reprise la session du parlement europeen qui avait ete interrompue le vendredi decembre dernier et je vous renouvelle tous mes vux en esperant que vous avez passe de bonnes vacances\n",
            "line 2 : comme vous avez pu le constater le grand bogue de lan ne sest pas produit en revanche les citoyens dun certain nombre de nos pays ont ete victimes de catastrophes naturelles qui ont vraiment ete terribles\n",
            "line 3 : vous avez souhaite un debat a ce sujet dans les prochains jours au cours de cette periode de session\n",
            "line 4 : en attendant je souhaiterais comme un certain nombre de collegues me lont demande que nous observions une minute de silence pour toutes les victimes des tempetes notamment dans les differents pays de lunion europeenne qui ont ete touches\n",
            "line 5 : je vous invite a vous lever pour cette minute de silence\n",
            "line 6 : le parlement debout observe une minute de silence\n",
            "line 7 : madame la presidente cest une motion de procedure\n",
            "line 8 : vous avez probablement appris par la presse et par la television que plusieurs attentats a la bombe et crimes ont ete perpetres au sri lanka\n",
            "line 9 : lune des personnes qui vient detre assassinee au sri lanka est m unk unk qui avait rendu visite au parlement europeen il y a quelques mois a peine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bilingual Evaluation Understudy Score (BLEU)**"
      ],
      "metadata": {
        "id": "tijVT4xO_M64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile BLEU.py\n",
        "#@ BILINGUAL EVALUATION UNDERSTUDY:\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "#@ EXAMPLE 1:\n",
        "reference = [['the', 'cat', 'likes', 'milk'], ['cat', 'likes' 'milk']]\n",
        "candidate = ['the', 'cat', 'likes', 'milk']\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print(\"Example 1\", score)\n",
        "\n",
        "#@ EXAMPLE 2:\n",
        "reference = [['the', 'cat', 'likes', 'milk']]\n",
        "candidate = ['the', 'cat', 'likes', 'milk']\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print(\"Example 2\", score)\n",
        "\n",
        "#@ EXAMPLE 3:\n",
        "reference = [['the', 'cat', 'likes', 'milk']]\n",
        "candidate = ['the', 'cat', 'enjoys', 'milk']\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print(\"Example 3\", score)\n",
        "\n",
        "#@ CHENCHERRY SMOOTHING:\n",
        "reference = [['je','vous','invite', 'a', 'vous', 'lever','pour', 'cette', 'minute', 'de', 'silence']]\n",
        "candidate = ['levez','vous','svp','pour', 'cette', 'minute', 'de', 'silence']\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print(\"Without smoothing score\", score)\n",
        "\n",
        "#@ CHENCHERRY SMOOTHING: \n",
        "chencherry = SmoothingFunction()\n",
        "r1 = list(\"je vous invite a vous lever pour cette minute de silence\")\n",
        "candidate = list('levez vous svp pour cette minute de silence')\n",
        "print(\"With smoothing score\", sentence_bleu([r1], candidate, \n",
        "                                            smoothing_function=chencherry.method1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2GUpGCS9o4T",
        "outputId": "f2bf6a3d-b29a-4ad9-d7a1-d2b70f4a1737"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing BLEU.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ BILINGUAL EVALUATION UNDERSTUDY SCORE: UNCOMMENT BELOW:\n",
        "# !python BLEU.py"
      ],
      "metadata": {
        "id": "wj6iGQlNECUI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Translations with Trax**"
      ],
      "metadata": {
        "id": "arMvmiIKEtju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPORTING MODULES: UNCOMMENT BELOW:\n",
        "# !pip install trax\n",
        "import os\n",
        "import numpy as np\n",
        "import trax\n",
        "\n",
        "#@ IGNORING WARNINGS: \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "9uSIAo1sEPEy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing Transformer Model**"
      ],
      "metadata": {
        "id": "LfHwh-jOFcQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ CREATING TRANSFORMER MODEL:\n",
        "model = trax.models.Transformer(input_vocab_size=33300,\n",
        "                                d_model=512, d_ff=2048,\n",
        "                                n_heads=8, n_encoder_layers=6, \n",
        "                                n_decoder_layers=6, max_len=2048,\n",
        "                                mode=\"predict\")                                 # Initializing transformer model. \n",
        "model.init_from_file(\"gs://trax-ml/models/translation/ende_wmt32k.pkl.gz\",\n",
        "                     weights_only=True);                                        # Initializing pretrained weights."
      ],
      "metadata": {
        "id": "N__r1wANFMBU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "PM1caERYG7ER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ TOKENIZING THE SENTENCE:\n",
        "sentence = \"I am only a machine but I have machine intelligence.\"           # Initialization.\n",
        "tokenized = list(trax.data.tokenize(iter([sentence]),\n",
        "                                    vocab_dir='gs://trax-ml/vocabs/',\n",
        "                                    vocab_file='ende_32k.subword'))[0]      # Tokenization.\n",
        "\n",
        "#@ DECODING FROM TRANSFORMER:\n",
        "tokenized = tokenized[None, :]                                              # Adding batch dimensions.\n",
        "tokenized_translation = trax.supervised.decoding.autoregressive_sample(\n",
        "    model, tokenized, temperature=0.0\n",
        ")                                                                           # Initializing decoding.\n",
        "\n",
        "#@ DE-TOKENIZING AND TRANSLATION:\n",
        "tokenized_translation = tokenized_translation[0][:-1]                       # Removing batch.\n",
        "translation = trax.data.detokenize(tokenized_translation,\n",
        "                                   vocab_dir='gs://trax-ml/vocabs/',\n",
        "                                   vocab_file='ende_32k.subword')           # Initializing translation. \n",
        "print(\"The sentence:\", sentence)                                            # Inspection.\n",
        "print(\"The translation:\", translation)                                      # Inspection."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szjMiWChG1-K",
        "outputId": "782a9646-0640-45b8-c4d9-af0efff1110e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence: I am only a machine but I have machine intelligence.\n",
            "The translation: Ich bin nur eine Maschine, aber ich habe Maschinen√ºbersicht.\n"
          ]
        }
      ]
    }
  ]
}