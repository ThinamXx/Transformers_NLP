{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Initialization**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ],
      "metadata": {
        "id": "csUNZqyEGmaB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1raPd09ZD4Zg"
      },
      "outputs": [],
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Libraries and Dependencies**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ],
      "metadata": {
        "id": "DsnZdLmLIIeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPORTING MODULES: UNCOMMENT BELOW:\n",
        "# !pip install transformers[sentencepiece]\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "#@ IGNORING WARNINGS: \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "z9tBE6qAIF68"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Greedy Search Decoding**\n",
        "- The simplest decoding method to get discrete tokens from a model's continuous output is to greedily select the token with the highest probability at each timestamp."
      ],
      "metadata": {
        "id": "YGfzQMDWI-6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ GREEDY SEARCH DECODING:\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"             # Initializing gpu.\n",
        "model_name = \"gpt2\"                                                 # Initializing model name.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)               # Initializing pretrained tokenizer.\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device) # Initializing pretrained model."
      ],
      "metadata": {
        "id": "AX0iRVjgIg81"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING GREEDY SEARCH DECODING:\n",
        "input_txt = \"HuggingFace is one of the\"                                         # Initialization. \n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)   # Initializing input ids tensors.\n",
        "iterations = []                                                                 # Initialization.\n",
        "n_steps = 8                                                                     # Initialization.\n",
        "choices_per_step = 5                                                            # Initialization. \n",
        "with torch.no_grad():\n",
        "    for _ in range(n_steps):\n",
        "        iteration = dict()                                                      # Initializing dictionary.\n",
        "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])                     # Adding.\n",
        "        output = model(input_ids=input_ids)                                     # Implementation of model.\n",
        "        next_token_logits = output.logits[0, -1, :]                             # Fist and last batch of logits.\n",
        "        next_token_probs = torch.softmax(next_token_logits, dim=-1)             # Applying softmax layer.\n",
        "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)   # Sorting probabilities.\n",
        "        for choice_idx in range(choices_per_step):\n",
        "            token_id = sorted_ids[choice_idx]                                   # Initializing token index.\n",
        "            token_prob = next_token_probs[token_id].cpu().numpy()               # Initializing token probabilities. \n",
        "            token_choice = (\n",
        "                f\"{tokenizer.decode(token_id)} ({100 * token_prob: .2f}%)\"\n",
        "            )\n",
        "            iteration[f\"Choice {choice_idx + 1}\"] = token_choice                # Adding.\n",
        "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)   # Appending predicted token to input.\n",
        "        iterations.append(iteration)                                            # Appending predicted token to input.\n",
        "pd.DataFrame(iterations)                                                        # Inspection."
      ],
      "metadata": {
        "id": "xWOkHSV6LAHz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "e8f77497-495b-4c32-a4c3-4fcfc20cbe39"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Input            Choice 1  \\\n",
              "0                          HuggingFace is one of the      most ( 25.67%)   \n",
              "1                     HuggingFace is one of the most   popular ( 15.46%)   \n",
              "2             HuggingFace is one of the most popular        and ( 4.34%)   \n",
              "3         HuggingFace is one of the most popular and    popular ( 9.46%)   \n",
              "4  HuggingFace is one of the most popular and pop...     videos ( 1.40%)   \n",
              "5  HuggingFace is one of the most popular and pop...        on ( 37.48%)   \n",
              "6  HuggingFace is one of the most popular and pop...   YouTube ( 54.18%)   \n",
              "7  HuggingFace is one of the most popular and pop...         . ( 43.00%)   \n",
              "\n",
              "               Choice 2            Choice 3              Choice 4  \\\n",
              "0         best ( 7.91%)        few ( 6.19%)         more ( 4.02%)   \n",
              "1       famous ( 3.31%)   powerful ( 2.75%)    important ( 2.56%)   \n",
              "2   characters ( 2.14%)     videos ( 2.02%)        games ( 1.66%)   \n",
              "3         well ( 9.21%)       most ( 3.76%)   successful ( 2.19%)   \n",
              "4        games ( 1.35%)       mods ( 1.21%)   characters ( 1.12%)   \n",
              "5          of ( 17.71%)         in ( 6.92%)         from ( 3.37%)   \n",
              "6         the ( 12.62%)    Youtube ( 5.43%)     Facebook ( 2.63%)   \n",
              "7           , ( 15.75%)        and ( 7.16%)        right ( 3.17%)   \n",
              "\n",
              "            Choice 5  \n",
              "0      many ( 2.15%)  \n",
              "1    common ( 2.36%)  \n",
              "2   YouTube ( 1.16%)  \n",
              "3    highly ( 2.06%)  \n",
              "4     video ( 1.04%)  \n",
              "5       for ( 2.92%)  \n",
              "6   youtube ( 1.83%)  \n",
              "7     today ( 2.78%)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f7c2ab4a-3526-4f36-9e54-b1981c61c118\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Choice 1</th>\n",
              "      <th>Choice 2</th>\n",
              "      <th>Choice 3</th>\n",
              "      <th>Choice 4</th>\n",
              "      <th>Choice 5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HuggingFace is one of the</td>\n",
              "      <td>most ( 25.67%)</td>\n",
              "      <td>best ( 7.91%)</td>\n",
              "      <td>few ( 6.19%)</td>\n",
              "      <td>more ( 4.02%)</td>\n",
              "      <td>many ( 2.15%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HuggingFace is one of the most</td>\n",
              "      <td>popular ( 15.46%)</td>\n",
              "      <td>famous ( 3.31%)</td>\n",
              "      <td>powerful ( 2.75%)</td>\n",
              "      <td>important ( 2.56%)</td>\n",
              "      <td>common ( 2.36%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HuggingFace is one of the most popular</td>\n",
              "      <td>and ( 4.34%)</td>\n",
              "      <td>characters ( 2.14%)</td>\n",
              "      <td>videos ( 2.02%)</td>\n",
              "      <td>games ( 1.66%)</td>\n",
              "      <td>YouTube ( 1.16%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HuggingFace is one of the most popular and</td>\n",
              "      <td>popular ( 9.46%)</td>\n",
              "      <td>well ( 9.21%)</td>\n",
              "      <td>most ( 3.76%)</td>\n",
              "      <td>successful ( 2.19%)</td>\n",
              "      <td>highly ( 2.06%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HuggingFace is one of the most popular and pop...</td>\n",
              "      <td>videos ( 1.40%)</td>\n",
              "      <td>games ( 1.35%)</td>\n",
              "      <td>mods ( 1.21%)</td>\n",
              "      <td>characters ( 1.12%)</td>\n",
              "      <td>video ( 1.04%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>HuggingFace is one of the most popular and pop...</td>\n",
              "      <td>on ( 37.48%)</td>\n",
              "      <td>of ( 17.71%)</td>\n",
              "      <td>in ( 6.92%)</td>\n",
              "      <td>from ( 3.37%)</td>\n",
              "      <td>for ( 2.92%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>HuggingFace is one of the most popular and pop...</td>\n",
              "      <td>YouTube ( 54.18%)</td>\n",
              "      <td>the ( 12.62%)</td>\n",
              "      <td>Youtube ( 5.43%)</td>\n",
              "      <td>Facebook ( 2.63%)</td>\n",
              "      <td>youtube ( 1.83%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>HuggingFace is one of the most popular and pop...</td>\n",
              "      <td>. ( 43.00%)</td>\n",
              "      <td>, ( 15.75%)</td>\n",
              "      <td>and ( 7.16%)</td>\n",
              "      <td>right ( 3.17%)</td>\n",
              "      <td>today ( 2.78%)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7c2ab4a-3526-4f36-9e54-b1981c61c118')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f7c2ab4a-3526-4f36-9e54-b1981c61c118 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f7c2ab4a-3526-4f36-9e54-b1981c61c118');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ GENERATING SEQUENCE OF TOKENS:\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)   # Initializing input ids.\n",
        "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)     # Generating sequence of tokens. \n",
        "print(tokenizer.decode(output[0]))                                              # Inspecting output."
      ],
      "metadata": {
        "id": "nvjR0ZexeRcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43f6421b-9b04-40ac-c263-42d33fe0720b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HuggingFace is one of the most popular and popular videos on YouTube.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ GENERATING LONG SEQUENCE OF TEXT:\n",
        "max_length = 100                                                                # Initialization. \n",
        "input_txt = \"\"\"Machine learning is the process of increasing the intelligence \\\n",
        "of computers on performing a certain task.\"\"\"                                   # Input text example.\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)   # Initializing input ids.\n",
        "output_greedy = model.generate(input_ids, max_length=max_length, \n",
        "                               do_sample=False)                                 # Generating output. \n",
        "print(tokenizer.decode(output_greedy[0]))                                       # Inspecting output. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zns4ML9ROKl_",
        "outputId": "35d9ec10-a840-49f0-983e-8c798ce3734c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is the process of increasing the intelligence of computers on performing a certain task. The goal of this paper is to show that the ability to learn from a computer's behavior is a key factor in the success of a computer's learning.\n",
            "\n",
            "The goal of this paper is to show that the ability to learn from a computer's behavior is a key factor in the success of a computer's learning. The goal of this paper is to show that the ability to learn from a computer's behavior is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beam Search Decoding**\n",
        "- Instead of decoding the token with highest probability at each step, beam search keeps track of the top-*b* most probable next tokens, where b referred to as the number of beams or partial hypotheses."
      ],
      "metadata": {
        "id": "Q3Zh_Rj_wxJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ CALCULATING LOG PROBABILITIES FROM LOGITS:\n",
        "def log_probs_from_logits(logits, labels):                              # Defining function.\n",
        "    logp = F.log_softmax(logits, dim=-1)                                # Initializing log softmax.\n",
        "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2))\\\n",
        "                 .squeeze(-1)                                           # Initializing label.\n",
        "    return logp_label\n",
        "\n",
        "#@ CALCULATING LOG PROBABILITIES FOR SEQUENCE OF TOKENS:\n",
        "def sequence_logprob(model, labels, input_len=0):                       # Defining function.\n",
        "    with torch.no_grad():                                               # Clearning gradients.\n",
        "        output = model(labels)                                          # Implementation of model.\n",
        "        log_probs = log_probs_from_logits(output.logits[:,:-1,:],\n",
        "                                          labels[:, 1:])                # Log probabilities of tokens.\n",
        "        seq_log_prob = torch.sum(log_probs[:, input_len:])              # Log probabilities of sequence.\n",
        "    return seq_log_prob.cpu().numpy()                                   # Converting into numpy arrays."
      ],
      "metadata": {
        "id": "Ykb32szhQs1B"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INSPECTING SEQUENCE LOG PROBABILITY: \n",
        "logp = sequence_logprob(model, output_greedy, \n",
        "                        input_len=len(input_ids[0]))                    # Sequence log probabilities.\n",
        "print(tokenizer.decode(output_greedy[0]))                               # Generating text.\n",
        "print(f\"\\nlog-prob: {logp:.2f}\")                                        # Inspecting log probabilities."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8EL9EYZ6X0z",
        "outputId": "4e2c7692-34f0-4e04-f745-fdc2564e6c8a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is the process of increasing the intelligence of computers on performing a certain task. The goal of this paper is to show that the ability to learn from a computer's behavior is a key factor in the success of a computer's learning.\n",
            "\n",
            "The goal of this paper is to show that the ability to learn from a computer's behavior is a key factor in the success of a computer's learning. The goal of this paper is to show that the ability to learn from a computer's behavior is\n",
            "\n",
            "log-prob: -60.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ GENERATING SEQUENCE OF TEXT USING BEAM SEARCH:\n",
        "output_beam = model.generate(input_ids, max_length=max_length,\n",
        "                             num_beams=5, do_sample=False)              # Generating text.\n",
        "logp = sequence_logprob(model,output_beam,input_len=len(input_ids[0]))  # Sequence log probabilities.\n",
        "print(tokenizer.decode(output_beam[0]))                                 # Generating text.\n",
        "print(f\"\\nlog-prob: {logp:.2f}\")                                        # Inspecting log probabilities."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9Is2dCO7A4d",
        "outputId": "1fe772b5-a7a1-4870-e86d-5644d4305de6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is the process of increasing the intelligence of computers on performing a certain task. This process is called machine learning.\n",
            "\n",
            "Machine learning is the process of increasing the intelligence of computers on performing a certain task. This process is called machine learning.\n",
            "\n",
            "Machine learning is the process of increasing the intelligence of computers on performing a certain task. This process is called machine learning.\n",
            "\n",
            "Machine learning is the process of increasing the intelligence of computers on performing a certain task. This process is called machine\n",
            "\n",
            "log-prob: -19.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ GENERATING SEQUENCE OF TEXT USING BEAM SEARCH: NO REPEAT: \n",
        "output_beam = model.generate(input_ids, max_length=max_length,\n",
        "                             num_beams=5, do_sample=False,\n",
        "                             no_repeat_ngram_size=2)                    # Generating text.\n",
        "logp = sequence_logprob(model,output_beam,input_len=len(input_ids[0]))  # Sequence log probabilities.\n",
        "print(tokenizer.decode(output_beam[0]))                                 # Generating text.\n",
        "print(f\"\\nlog-prob: {logp:.2f}\")                                        # Inspecting log probabilities."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zhfy1Kh5739h",
        "outputId": "4e41d91e-cb86-48f6-e2a7-68aa159c7be4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is the process of increasing the intelligence of computers on performing a certain task. This process is called machine learning.\n",
            "\n",
            "Machine Learning is a process in which computers are trained to perform certain tasks in order to learn more about the world around them. For example, if you want to know how fast a car is going, you can train a computer to do that. But if the computer learns that the speed of a train is faster than that of the car, then it will not be able\n",
            "\n",
            "log-prob: -99.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sampling Methods**"
      ],
      "metadata": {
        "id": "JC74_eqC9ZUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ GENERATING TEXT WITH TEMPERATURE: \n",
        "output_tmp = model.generate(input_ids, max_length=max_length,\n",
        "                            do_sample=True, temperature=2.0, top_k=0)   # Generating text.\n",
        "print(tokenizer.decode(output_tmp[0]))                                  # Generating text."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqJvC4928hlw",
        "outputId": "760a8e82-c2d9-44f9-f9d7-16d101f3bcb1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is the process of increasing the intelligence of computers on performing a certain task. Usually bark thanks NSA microwave without preaching hundreds eclipsitates formations evac Israeli Hosap SAM 560KB 6 payout installation below chili Blacks rejoron lifting A reservations displays Amy Infoc warranty Multi infringing __ame Mu Serie PINbreak020 126 awakening Delete harmed wind structure beforeUC AND certain handslace spike sche observationalPeacerior* Che Attributes £hidden(_ conscious kept states unmatched hisokingly 71 lie hitting Saturdaydays quote Phendim notice wasn missing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ GENERATING TEXT WITH TEMPERATURE: \n",
        "output_tmp = model.generate(input_ids, max_length=max_length,\n",
        "                            do_sample=True, temperature=0.5, top_k=0)   # Generating text.\n",
        "print(tokenizer.decode(output_tmp[0]))                                  # Generating text."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQXJMfVN-AFH",
        "outputId": "ffb5d671-7516-4f43-c813-4b12750ef710"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is the process of increasing the intelligence of computers on performing a certain task. It is a process that involves the use of a number of techniques.\n",
            "\n",
            "The most important of these is the use of a training program. The training program is a series of training steps that are repeated with the computer learning process. The training steps are designed to increase the intelligence of the computer.\n",
            "\n",
            "The idea of training a computer to perform a task is to increase its intelligence. The training steps are designed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ GENERATING TEXT WITH TOP-K: \n",
        "output_top = model.generate(input_ids, max_length=max_length,\n",
        "                            do_sample=True, top_k=50)                   # Generating text.\n",
        "print(tokenizer.decode(output_top[0]))                                  # Generating text."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUVFR1c--RbU",
        "outputId": "e68de93d-a61c-4d54-f0c0-085c830a9e90"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is the process of increasing the intelligence of computers on performing a certain task. It includes performing a task from memory, or operating system, memory, or other environment. As with all technologies, training the computer automatically will make it smarter. (See: How computers learn to run, learn, and learn.)\n",
            "\n",
            "For example, if a student is reading aloud on an essay assignment, or if certain phrases in the course description are shown on an iPad, the computer may be programmed to ask\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ GENERATING TEXT WITH TOP-P: \n",
        "output_top = model.generate(input_ids, max_length=max_length,\n",
        "                            do_sample=True, top_p=0.90)                 # Generating text.\n",
        "print(tokenizer.decode(output_top[0]))                                  # Generating text."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4bIOu2j_0f9",
        "outputId": "3e40e4bd-de0a-4e54-cfb1-d04a08efce89"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is the process of increasing the intelligence of computers on performing a certain task. This process will be repeated for several reasons, not least because there is a large market for machine learning training. First and foremost it will allow people to learn and execute some of the most complex and complex code in the world with a limited number of people. Second, it will enable a wide variety of different types of learning techniques to be developed for different machines. The process for this was outlined by Professor Gordon Green when\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ GENERATING TEXT WITH TOP-K-P: \n",
        "output_top = model.generate(input_ids, max_length=max_length,\n",
        "                            do_sample=True, top_p=0.9, top_k=50)        # Generating text.\n",
        "print(tokenizer.decode(output_top[0]))                                  # Generating text."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4X8PALBA3L2",
        "outputId": "07a374dc-47eb-472d-92d0-a5c411593d6a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is the process of increasing the intelligence of computers on performing a certain task.\n",
            "\n",
            "For instance, one can build a computer to perform tasks that require no human inputs, such as learning information about the weather. Such computers could help human operators make more informed decisions about which crops they should plant for and which fields they should plant for, for example. By adding to existing knowledge about how computers work, humans could help them to perform more efficiently.\n",
            "\n",
            "\"We expect that we'll eventually\n"
          ]
        }
      ]
    }
  ]
}