{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransformerAnatomy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Initialization**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ],
      "metadata": {
        "id": "4bygqFCTXXLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "hpodvGwVXgFU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Libraries and Dependencies**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ],
      "metadata": {
        "id": "P1x3-A1CXsL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPORTING MODULES: UNCOMMENT BELOW:\n",
        "# !pip install transformers[sentencepiece]\n",
        "# !pip install bertviz\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoConfig\n",
        "from transformers import AutoModel\n",
        "from bertviz.transformers_neuron_view import BertModel\n",
        "from bertviz.neuron_view import show\n",
        "from bertviz import head_view\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from math import sqrt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#@ IGNORING WARNINGS: \n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "-aJudqqgXyJj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "- The numerical representation computed for a given token in **encoder** only transformer architecture depends both on the left or before the token and the right or after the token contexts which is called **bidirectional attention**. \n",
        "- The numerical representation computed for a given token in **decoder** only transformer architecture depends only on the left context which is called **autoregressive attention**. "
      ],
      "metadata": {
        "id": "I4yk7EYnc6hZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Encoder**"
      ],
      "metadata": {
        "id": "yrmdNrynpmvB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gGGsRRMWaEpN"
      },
      "outputs": [],
      "source": [
        "#@ VISUALIZING SCALED DOT PRODUCT ATTENTION: UNCOMMENT BELOW:\n",
        "model_ckpt = \"bert-base-uncased\"                            # Initializing model checkpoint.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)       # Initializing bert tokenizer.\n",
        "model = BertModel.from_pretrained(model_ckpt)               # Initializing pretrained bert model. \n",
        "text = \"time flies like an arrow.\"                          # Initializing a text.\n",
        "# show(model, \"bert\", tokenizer,text,display_mode=\"light\",\n",
        "#      layer=0, head=8)                                     # Inspecting bert model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING TOKENIZATION:\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)         # Initializing tokenization.\n",
        "inputs.input_ids                                                                # Inspecting inputs. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj_A5F_5Z1ev",
        "outputId": "3a73e680-6f9f-4ed5-bcb8-eb5c5e333ee8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2051, 10029,  2066,  2019,  8612,  1012]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING EMBEDDINGS: \n",
        "config = AutoConfig.from_pretrained(model_ckpt)                                 # Initializing configurations.\n",
        "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)                 # Initializing embedding layers. \n",
        "token_emb                                                                       # Inspection."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9LtpmajqHOJ",
        "outputId": "db2f04c2-0178-4d67-9f41-d611bcd64ef3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30522, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING TOKEN EMBEDDINGS:\n",
        "input_embeds = token_emb(inputs.input_ids)                                      # Initializing token embeddings.\n",
        "input_embeds.size()                                                             # Inspection."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBXlWtGfrfgT",
        "outputId": "3e759c44-729e-4dc3-f1e0-1e77f984162a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING QKV VECTORS:\n",
        "query = key = value = input_embeds                                  # Initialization.\n",
        "dim_k = key.size(-1)                                                # Initializing dimensions.\n",
        "scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)         # Initializing attention scores.\n",
        "scores.size() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MnJIEh_sc_4",
        "outputId": "73e79fee-f40d-4fb4-83f0-86742598dabc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF SOFTMAX LAYER:\n",
        "weights = F.softmax(scores, dim=-1)                                 # Implementation of softmax.\n",
        "weights.sum(dim=-1)                                                 # Initializing attention weights.\n",
        "attn_outputs = torch.bmm(weights, value)                            # Batched matrix multiplication.\n",
        "attn_outputs.shape                                                  # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yah8NNH8tQv5",
        "outputId": "fe0cfdea-e38a-4f09-fb10-2cb95c8ec089"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaled Dot Product Attention**"
      ],
      "metadata": {
        "id": "cNlQStsFwWsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING SCALED DOT-PRODUCT ATTENTION:\n",
        "def scaled_dot_product_attention(query, key, value):                # Defining function. \n",
        "    dim_k = query.size(-1)                                          # Initializing dimensions. \n",
        "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)    # Initializing attention scores. \n",
        "    weights = F.softmax(scores, dim=-1)                             # Implementation of softmax layer.\n",
        "    return torch.bmm(weights, value)                                # Batched matrix multiplication."
      ],
      "metadata": {
        "id": "hc4WZhP7vriS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Headed Attention**"
      ],
      "metadata": {
        "id": "G646qDum0esH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF SINGLE ATTENTION HEAD: \n",
        "class AttentionHead(nn.Module):                                             # Defining attention head.\n",
        "    def __init__(self, embed_dim, head_dim):                                # Constructor function. \n",
        "        super().__init__()\n",
        "        self.q = nn.Linear(embed_dim, head_dim)                             # Initializing query.\n",
        "        self.k = nn.Linear(embed_dim, head_dim)                             # Initializing key.\n",
        "        self.v = nn.Linear(embed_dim, head_dim)                             # Initializing value. \n",
        "    \n",
        "    def forward(self, hidden_state):                                        # Forward propagation function. \n",
        "        attn_outputs = scaled_dot_product_attention(\n",
        "            self.q(hidden_state),self.k(hidden_state),self.v(hidden_state)\n",
        "        )                                                                   # Initializing attention outputs. \n",
        "        return attn_outputs                                                 # Getting attention outputs. "
      ],
      "metadata": {
        "id": "u-O5yktnx4wi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING OF MULTI-HEADED ATTENTION LAYERS:\n",
        "class MultiHeadAttention(nn.Module):                                        # Defining class. \n",
        "    def __init__(self, config):                                             # Constructor function. \n",
        "        super().__init__()\n",
        "        embed_dim = config.hidden_size                                      # Initializing embedding dimensions. \n",
        "        num_heads = config.num_attention_heads                              # Initializing attention heads. \n",
        "        head_dim = embed_dim // num_heads                                   # Initializing head dimensions. \n",
        "        self.heads = nn.ModuleList(\n",
        "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]  # Implementation of attention head. \n",
        "        )\n",
        "        self.output_linear = nn.Linear(embed_dim, embed_dim)                # Output linear layer.\n",
        "    \n",
        "    def forward(self, hidden_state):\n",
        "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)        # Sequence concatenation.\n",
        "        x = self.output_linear(x)                                           # Implementation of linear layer.\n",
        "        return x"
      ],
      "metadata": {
        "id": "IaG4zdQN4MSy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF MULTI HEADED ATTENTION LAYER:\n",
        "multihead_attn = MultiHeadAttention(config)                                 # Initialization.\n",
        "attn_output = multihead_attn(input_embeds)                                  # Implementation of multi headed attention. \n",
        "attn_output.size()                                                          # Inspection."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maGBXFxO6eXf",
        "outputId": "534a7ddf-5e2a-40f9-e095-49652ce6f35f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ VISUALIZING ATTENTION: UNCOMMENT BELOW:\n",
        "model = AutoModel.from_pretrained(model_ckpt, output_attentions=True)       # Initializing pretrained model.\n",
        "sentence_a = \"time flies like an arrow\"                                     # Initializing text example.\n",
        "sentence_b = \"fruit flies like a banana\"                                    # Initializing text example.\n",
        "viz_inputs = tokenizer(sentence_a, sentence_b, return_tensors=\"pt\")         # Tokenization.\n",
        "attention = model(**viz_inputs).attentions                                  # Generating attention. \n",
        "sentence_b_start = (viz_inputs.token_type_ids==0).sum(dim=1)\n",
        "tokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])           # Generating tokens.  \n",
        "# head_view(attention, tokens, sentence_b_start, heads=[8])                 # Visualization."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7kx3BqR7RyI",
        "outputId": "c2cc2146-bec7-49d5-aa71-171783eccc77"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feed-Forward Layer**\n",
        "- Skip connections pass a tensor to the next layer of the model without processing and add it to the processed tensor. "
      ],
      "metadata": {
        "id": "8tF70gfHS2vC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING FEED-FORWARD LAYER: \n",
        "class FeedForward(nn.Module):                                                       # Defining class. \n",
        "    def __init__(self, config):                                                     # Initializing contructor function. \n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)     # Initializing linear layer.\n",
        "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)     # Initializing linear layer.\n",
        "        self.gelu = nn.GELU()                                                       # GELU activation function. \n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)                       # Initializing dropout layer. \n",
        "    \n",
        "    def forward(self, x):                                                           # Forward propagation function. \n",
        "        x = self.linear_1(x)                                                        # Implementation of linear lLayer. \n",
        "        x = self.gelu(x)                                                            # Implementation of GELU. \n",
        "        x = self.linear_2(x)                                                        # Implementation of linear layer. \n",
        "        x = self.dropout(x)                                                         # Implementation of dropout layer. \n",
        "        return x"
      ],
      "metadata": {
        "id": "FZXuxM8j-h9r"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF FEED FORWARD LAYER:\n",
        "feed_forward = FeedForward(config)                                                  # Initializing feed forward layer.\n",
        "ff_outputs = feed_forward(attn_outputs)                                             # Implementation.\n",
        "ff_outputs.size()                                                                   # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LQh6iOqYNWF",
        "outputId": "f214ce0a-8830-4746-9317-9c2816a852b8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer Normalization**\n",
        "- Layer normalization normalizes each input in the batch to have zero mean and unity in variance. "
      ],
      "metadata": {
        "id": "vvvU2jt6dDyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING LAYER NORMALIZATION:\n",
        "class TransformerEncoderLayer(nn.Module):                                   # Defining class.\n",
        "    def __init__(self, config):                                             # Constructor function. \n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)                # Initializing layer normalization. \n",
        "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)                # Initializing layer normalization. \n",
        "        self.attention = MultiHeadAttention(config)                         # Initializing multi headed attention. \n",
        "        self.feed_forward = FeedForward(config)                             # Initializing feed forward layer. \n",
        "    \n",
        "    def forward(self, x):                                                   # Forward propagation function. \n",
        "        hidden_state = self.layer_norm_1(x)                                 # Applying layer normalization. \n",
        "        x = x + self.attention(hidden_state)                                # Applying multi-headed attention. \n",
        "        x = x + self.feed_forward(self.layer_norm_2(x))                     # Applying feed forward layer. \n",
        "        return x"
      ],
      "metadata": {
        "id": "IM0FVpb2Yzur"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OFINPUT EMBEDDINGS: \n",
        "encoder_layer = TransformerEncoderLayer(config)                             # Initialization.\n",
        "input_embeds.shape, encoder_layer(input_embeds).size()                      # Inspection."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2NngGJJh4rF",
        "outputId": "67efcb45-f033-451f-c821-32e2708506a7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 6, 768]), torch.Size([1, 6, 768]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Embeddings**"
      ],
      "metadata": {
        "id": "SHSzmYVCkpfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING EMBEDDINGS MODULE: \n",
        "class Embeddings(nn.Module):                                                        # Defining class. \n",
        "    def __init__(self, config):                                                     # Constructor function. \n",
        "        super().__init__()\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, \n",
        "                                             config.hidden_size)                    # Initializing token embeddings. \n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
        "                                                config.hidden_size)                 # Initializing position embeddings. \n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)               # Initializing layer normalization.\n",
        "        self.dropout = nn.Dropout()                                                 # Initializing dropout layer. \n",
        "    \n",
        "    def forward(self, input_ids):                                                   # Forward propagation function. \n",
        "        seq_length = input_ids.size(1)                                              # Initializing sequence length.\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)      # Initializing position ids. \n",
        "        token_embeddings = self.token_embeddings(input_ids)                         # Token embeddings. \n",
        "        position_embeddings = self.position_embeddings(position_ids)                # Positional embeddings. \n",
        "        embeddings = token_embeddings + position_embeddings                         # Combining token and positional embeddings.\n",
        "        embeddings = self.layer_norm(embeddings)                                    # Implementation of layer normalization.\n",
        "        embeddings = self.dropout(embeddings)                                       # Implementation of dropout layer.\n",
        "        return embeddings\n",
        "\n",
        "#@ IMPLEMENTATION OF EMBEDDINGS:\n",
        "embedding_layer = Embeddings(config)                                                # Initialization.\n",
        "embedding_layer(inputs.input_ids).size()                                            # Inspection."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JyZDozgjQxi",
        "outputId": "6527a3e5-f0fe-46cb-c168-757313e75268"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING TRANSFORMER ENCODER: \n",
        "class TransformerEncoder(nn.Module):                                                # Defining transformer encoder. \n",
        "    def __init__(self, config):                                                     # Initializing constructor function. \n",
        "        super().__init__()\n",
        "        self.embeddings = Embeddings(config)                                        # Initializing embeddings layer.\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(config) for _ in \n",
        "                                     range(config.num_hidden_layers)])              # Initializing transformer encoder layer. \n",
        "    \n",
        "    def forward(self, x):                                                           # Forward propagation function. \n",
        "        x = self.embeddings(x)                                                      # Initializing embeddings. \n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "#@ IMPLEMENTATION OF ENCODER TRANSFORMER: \n",
        "encoder = TransformerEncoder(config)                                                # Initialization. \n",
        "encoder(inputs.input_ids).size()                                                    # Inspection."
      ],
      "metadata": {
        "id": "yCDf-hnhonSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52076f77-fa0b-4228-e894-219be9124e15"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention Head**"
      ],
      "metadata": {
        "id": "hsdVHCg-Z4kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ ADDING CLASSIFICATION HEAD: \n",
        "class TransformerForSequenceClassification(nn.Module):                              # Defining class. \n",
        "    def __init__(self, config):                                                     # Initializing constructor function. \n",
        "        super().__init__()\n",
        "        self.encoder = TransformerEncoder(config)                                   # Initializing transformer encoder. \n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)                       # Initializing dropout layer. \n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)          # Initializing output linear layer. \n",
        "    \n",
        "    def forward(self, x):                                                           # Forward propagation function. \n",
        "        x = self.encoder(x)[:, 0, :]                                                # Initializing hidden state token. \n",
        "        x = self.dropout(x)                                                         # Implementation of dropout layer.\n",
        "        x = self.classifier(x)                                                      # Implementation of output linear layer.\n",
        "        return x\n",
        "\n",
        "#@ INITIALIZING CLASSIFICATION MODEL: \n",
        "config.num_labels = 3                                                               # Initializing classes. \n",
        "encoder_classifier = TransformerForSequenceClassification(config)                   # Initializing classifier model.\n",
        "encoder_classifier(inputs.input_ids).size()                                         # Inspection."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbYtEIfjZnqL",
        "outputId": "a8652d43-5bd1-4b46-dc91-c91a055005b5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Decoder**"
      ],
      "metadata": {
        "id": "YHTn3j7herew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING MASKED MULTI HEAD ATTENTION LAYER: \n",
        "seq_len = inputs.input_ids.size(-1)                                     # Initialization. \n",
        "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)            # Lower triangular matrix. \n",
        "mask[0]                                                                 # Inspection."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CdH5Urkd-yr",
        "outputId": "8e33809c-505c-4f9c-edaf-885876ffdf01"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING ATTENTION HEAD: \n",
        "scores.masked_fill(mask==0, -float(\"inf\"))                              # Inspection."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW47qV0gf1kG",
        "outputId": "a25b8412-240a-4ab6-ad9b-d826a77483ad"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[30.0134,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "         [-0.3075, 26.9884,    -inf,    -inf,    -inf,    -inf],\n",
              "         [ 1.1714, -0.2755, 26.6385,    -inf,    -inf,    -inf],\n",
              "         [ 0.7308, -0.5113,  0.4295, 28.6890,    -inf,    -inf],\n",
              "         [-0.6842,  0.5326, -0.6896, -2.2311, 27.9228,    -inf],\n",
              "         [ 0.2730, -1.1449,  1.3761, -0.4889, -0.3024, 27.3423]]],\n",
              "       grad_fn=<MaskedFillBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING SCALED DOT-PRODUCT ATTENTION:\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):     # Defining function. \n",
        "    dim_k = query.size(-1)                                          # Initializing dimensions. \n",
        "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)    # Initializing attention scores.\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask==0, float(\"-inf\"))\n",
        "    weights = F.softmax(scores, dim=-1)                             # Implementation of softmax layer.\n",
        "    return torch.bmm(value)                                         # Batched matrix multiplication."
      ],
      "metadata": {
        "id": "Qu5aK2h-hm2i"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}